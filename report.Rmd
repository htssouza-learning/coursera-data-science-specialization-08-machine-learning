---
title: 'Practical Machine Learning Course Project - Weight Lifting Exercise Classification'
author: "Henrique Souza (github.com/htssouza)"
output:
  html_document: default
  pdf_document:
    pandoc_args:
    - +RTS
    - -K64m
    - -RTS
---

# Executive Summary

This report describes the process behind the creation of a Machine Learning Model
used to classify weight lifting exercise (unilateral dumbbell biceps curling) in classes:

  * A: exactly according to the specification;
  * B: throwing the elbows to the front;
  * C: lifting the dumbbell only halfway; 
  * D: lowering the dumbbell only halfway; 
  * E: throwing the hips to the front.

More about the research and data used can be found on the following website:
http://groupware.les.inf.puc-rio.br/har#weight_lifting_exercises#ixzz6CzLP0YxO

Our accuracy measured on a validation dataset (part of training) was higher than 95%.

# Exploratory Data Analysis

Our data source urls:

```{r echo = TRUE}
TRAINING_SOURCE_FILE_URL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
TESTING_SOURCE_FILE_URL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
```

```{r echo = FALSE, message = FALSE}
# libraries and data download
library(caret)
library(dplyr)
library(ggplot2)

set.seed(12345)

WORKING_DIRECTORY <- getwd()
TRAINING_FILE_PATH <- file.path(WORKING_DIRECTORY, "pml-training.csv")
TESTING_FILE_PATH <- file.path(WORKING_DIRECTORY, "pml-testing.csv")

if (! file.exists(TRAINING_FILE_PATH)) {
    download.file(url = TRAINING_SOURCE_FILE_URL, destfile = TRAINING_FILE_PATH, method = "curl")
}
if (! file.exists(TESTING_FILE_PATH)) {
    download.file(url = TESTING_SOURCE_FILE_URL, destfile = TESTING_FILE_PATH, method = "curl")
}
```

Loading and splitting the data (training, validating and testing):

```{r echo = TRUE}
NA_STRINGS <- c("NA","#DIV/0!")
training <- read.csv(TRAINING_FILE_PATH, na.strings = NA_STRINGS)
testing <- read.csv(TESTING_FILE_PATH, na.strings = NA_STRINGS)
in.training <- createDataPartition(y = training$class, p = 0.7, list = FALSE)
validating <- training[-in.training, ]
training <- training[in.training, ]
```

How our original training dataset looks like:

```{r echo = TRUE}
dim(training)
print(table(training$class))
```

Checking the presence of NAs per variable:

```{r echo = FALSE, message = FALSE}
na.per.column <- sapply(training, function(x) sum(is.na(x)) / length(x))
na.per.column <- as.numeric(na.per.column[order(-na.per.column)])
na.stats <- table(cut(na.per.column, 20))
na.stats <- na.stats[na.stats > 0]
```

```{r echo = TRUE}
na.stats
```

A large number of variables have 95% or more of NAs values.
These variables will be ignored on our models.

We will always remove other variables that should not be related to the response,
such as timestamp and the name of the atlet.

```{r echo = TRUE}
unwanted.columns <- c("X", "raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp", "new_window", "num_window", "user_name")
```

```{r echo = FALSE, message = FALSE}
na.per.column <- sapply(training, function(x) sum(is.na(x)) / length(x))
na.per.column <- na.per.column[order(-na.per.column)]
almost.empty.columns <- names(na.per.column[as.numeric(na.per.column) >= 0.95])

training <- training %>% 
  select(-unwanted.columns) %>% 
  select(-almost.empty.columns) 
```

You can see plots and more details about the training dataset on the Appendix.

New dimensions of our training dataset:

```{r echo = TRUE}
dim(training)
```

# Modelling

PCA:

```{r echo = TRUE}
model.pca <- preProcess(training, method = "pca", thresh = 80/100)
training.pcs <- predict(model.pca, training)
validating.pcs <- predict(model.pca, validating)
```

Columns of our training dataset only with the principal components:

```{r echo = TRUE}
ncol(select(training.pcs, -classe))
```

We are training 2 models: Random Forest and GBM.

```{r echo = TRUE}
model.rf <- train(classe ~ ., method = "rf", data = training.pcs, trControl = trainControl(method="cv"), number = 3)
model.gbm <- train(classe ~ ., method = "gbm", data = training.pcs, verbose = FALSE)
```

# Model selection

Accuracy of our models on the training dataset:

```{r echo = TRUE}
confusionMatrix(training.pcs$classe, predict(model.rf))$overall['Accuracy']
confusionMatrix(training.pcs$classe, predict(model.gbm))$overall['Accuracy']
```

Now, checking the accuracy on the validation dataset:

```{r echo = FALSE}
validating.rf <- predict(model.rf, newdata = validating.pcs)
validating.gbm <- predict(model.gbm, newdata = validating.pcs)
```

```{r echo = TRUE}
confusionMatrix(validating.pcs$class, validating.rf)$overall['Accuracy']
confusionMatrix(validating.pcs$class, validating.gbm)$overall['Accuracy']
```

As the Random Forest model is already achieving a very high accuracy, no stacking/ensemble will be performed.

# Prediction

```{r echo = TRUE}
testing.pcs <- predict(model.pca, testing)
result <- predict(model.rf, newdata = testing.pcs)
result
```

# Conclusion

We were able to build a very accurate model for our classification problem.

Using PCA we were able to reduce the model to 12 variables (components).

# Appendix

Variables there are being ignored:

```{r echo = TRUE}
unwanted.columns
almost.empty.columns
```

With the removal of variables with too many NAs, there are no more NearZeroVars as well:

```{r echo = TRUE}
nzv <- nearZeroVar(training, saveMetrics = TRUE)
print(nzv[nzv$nzv,])
```

Boxplot for each numeric variable per classe:

```{r echo = FALSE, message = FALSE}
numeric.columns <- names(training)[as.character(sapply(training, function(x) { class(x) })) %in% c("numeric", "integer")]
OUTLIER_IQR_FACTOR <- 2
for(column.name in numeric.columns) {
    df <- select(training, classe, var = !!column.name)
    df <- filter(df, !is.na(var))
    print(ggplot(data = df, aes(y = var, fill = classe)) +
              geom_boxplot(coef = OUTLIER_IQR_FACTOR,
                           outlier.colour = "red",
                           outlier.shape = 16,
                           outlier.size = 2,
                           notch = FALSE) +
              ggtitle(column.name))
}
```

